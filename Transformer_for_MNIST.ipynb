{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea12e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import math, time\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f3aa3483",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts image to [1, 28, 28] and scales to [0, 1]\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_data, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74759dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x300 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a few sample images and labels\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# Show first 6 images in the batch\n",
    "plt.figure(figsize=(10, 3))\n",
    "for i in range(6):\n",
    "    plt.subplot(1, 6, i+1)\n",
    "    plt.imshow(example_data[i][0], cmap='gray')\n",
    "    plt.title(f\"Label: {example_targets[i].item()}\")\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7670b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([64, 1, 28, 28])\n",
      "Label batch shape: torch.Size([64])\n",
      "Number of training images: 60000\n",
      "Number of test images: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Image batch shape:\", example_data.shape)   # [64, 1, 28, 28]\n",
    "print(\"Label batch shape:\", example_targets.shape)  # [64]\n",
    "\n",
    "print(\"Number of training images:\", len(train_data))\n",
    "print(\"Number of test images:\", len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4bdf880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8f200d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=28, model_dim=128, num_heads=4, num_layers=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_enc = PositionalEncoding(model_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dim_feedforward=256,  batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)              # (B, 28, 28)\n",
    "        x = self.embedding(x)         # (B, 28, model_dim)\n",
    "        x = self.pos_enc(x)           # (B, 28, model_dim)\n",
    "        x = x.permute(1, 0, 2)        # (28, B, model_dim)\n",
    "        x = self.transformer(x)       # (28, B, model_dim)\n",
    "        x = x.mean(dim=0)             # (B, model_dim)\n",
    "        return self.classifier(x)     # (B, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=28, model_dim=128, num_heads=4, num_layers=2, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_enc = PositionalEncoding(model_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dim_feedforward=256,  batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Linear(model_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1)              # (B, 28, 28)\n",
    "        x = self.embedding(x)         # (B, 28, model_dim)\n",
    "        x = self.pos_enc(x)           # (B, 28, model_dim)\n",
    "        x = x.permute(1, 0, 2)        # (28, B, model_dim)\n",
    "        x = self.transformer(x)       # (28, B, model_dim)\n",
    "        x = x.mean(dim=0)             # (B, model_dim)\n",
    "        return self.classifier(x)     # (B, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e68866be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MNISTTransformer().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ee6f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6f0ca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "    print(f\"Accuracy:  {acc * 100:.2f}%\")\n",
    "    print(f\"F1 Score:  {f1:.4f}\")\n",
    "    print(f\"Recall:    {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fc6cd50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 completed in 80.16 seconds:\n",
      "Accuracy:  92.87%\n",
      "F1 Score:  0.9291\n",
      "Recall:    0.9287\n",
      "Precision: 0.9322\n",
      "\n",
      "Epoch 2 completed in 81.71 seconds:\n",
      "Accuracy:  95.32%\n",
      "F1 Score:  0.9531\n",
      "Recall:    0.9532\n",
      "Precision: 0.9536\n",
      "\n",
      "Epoch 3 completed in 77.97 seconds:\n",
      "Accuracy:  95.30%\n",
      "F1 Score:  0.9530\n",
      "Recall:    0.9530\n",
      "Precision: 0.9538\n",
      "\n",
      "Epoch 4 completed in 79.41 seconds:\n",
      "Accuracy:  95.97%\n",
      "F1 Score:  0.9597\n",
      "Recall:    0.9597\n",
      "Precision: 0.9599\n",
      "\n",
      "Epoch 5 completed in 77.12 seconds:\n",
      "Accuracy:  96.19%\n",
      "F1 Score:  0.9619\n",
      "Recall:    0.9619\n",
      "Precision: 0.9620\n",
      "\n",
      "✅ Training completed in 417.95 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_start = time.time()  # ⏱ Start total training timer\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()  # ⏱ Start epoch timer\n",
    "\n",
    "    train()  # Train for one epoch\n",
    "\n",
    "    duration = time.time() - start  # Time taken for this epoch\n",
    "    print(f\"\\nEpoch {epoch+1} completed in {duration:.2f} seconds:\")\n",
    "\n",
    "    evaluate()  # Evaluate after each epoch if needed\n",
    "\n",
    "total_duration = time.time() - total_start  # Total time for all epochs\n",
    "print(f\"\\n✅ Training completed in {total_duration:.2f} seconds.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2dd36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd90e00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ccde0a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining function to count parameters\n",
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"\\nTotal trainable parameters: {total:,}\\n\")\n",
    "    \n",
    "    print(f\"{'Layer':<60} {'Param #':>12}\")\n",
    "    print(\"=\"*75)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f\"{name:<60} {param.numel():>12}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fecc5ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total trainable parameters: 269,962\n",
      "\n",
      "Layer                                                             Param #\n",
      "===========================================================================\n",
      "embedding.weight                                                     3584\n",
      "embedding.bias                                                        128\n",
      "transformer.layers.0.self_attn.in_proj_weight                       49152\n",
      "transformer.layers.0.self_attn.in_proj_bias                           384\n",
      "transformer.layers.0.self_attn.out_proj.weight                      16384\n",
      "transformer.layers.0.self_attn.out_proj.bias                          128\n",
      "transformer.layers.0.linear1.weight                                 32768\n",
      "transformer.layers.0.linear1.bias                                     256\n",
      "transformer.layers.0.linear2.weight                                 32768\n",
      "transformer.layers.0.linear2.bias                                     128\n",
      "transformer.layers.0.norm1.weight                                     128\n",
      "transformer.layers.0.norm1.bias                                       128\n",
      "transformer.layers.0.norm2.weight                                     128\n",
      "transformer.layers.0.norm2.bias                                       128\n",
      "transformer.layers.1.self_attn.in_proj_weight                       49152\n",
      "transformer.layers.1.self_attn.in_proj_bias                           384\n",
      "transformer.layers.1.self_attn.out_proj.weight                      16384\n",
      "transformer.layers.1.self_attn.out_proj.bias                          128\n",
      "transformer.layers.1.linear1.weight                                 32768\n",
      "transformer.layers.1.linear1.bias                                     256\n",
      "transformer.layers.1.linear2.weight                                 32768\n",
      "transformer.layers.1.linear2.bias                                     128\n",
      "transformer.layers.1.norm1.weight                                     128\n",
      "transformer.layers.1.norm1.bias                                       128\n",
      "transformer.layers.1.norm2.weight                                     128\n",
      "transformer.layers.1.norm2.bias                                       128\n",
      "classifier.weight                                                    1280\n",
      "classifier.bias                                                        10\n"
     ]
    }
   ],
   "source": [
    "model = MNISTTransformer().to(DEVICE)\n",
    "count_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6a49de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"full_precision.pth\")\n",
    "state_dict = torch.load(\"full_precision.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bdfdb230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight\n",
      "embedding.bias\n",
      "transformer.layers.0.self_attn.in_proj_weight\n",
      "transformer.layers.0.self_attn.in_proj_bias\n",
      "transformer.layers.0.self_attn.out_proj.weight\n",
      "transformer.layers.0.self_attn.out_proj.bias\n",
      "transformer.layers.0.linear1.weight\n",
      "transformer.layers.0.linear1.bias\n",
      "transformer.layers.0.linear2.weight\n",
      "transformer.layers.0.linear2.bias\n",
      "transformer.layers.0.norm1.weight\n",
      "transformer.layers.0.norm1.bias\n",
      "transformer.layers.0.norm2.weight\n",
      "transformer.layers.0.norm2.bias\n",
      "transformer.layers.1.self_attn.in_proj_weight\n",
      "transformer.layers.1.self_attn.in_proj_bias\n",
      "transformer.layers.1.self_attn.out_proj.weight\n",
      "transformer.layers.1.self_attn.out_proj.bias\n",
      "transformer.layers.1.linear1.weight\n",
      "transformer.layers.1.linear1.bias\n",
      "transformer.layers.1.linear2.weight\n",
      "transformer.layers.1.linear2.bias\n",
      "transformer.layers.1.norm1.weight\n",
      "transformer.layers.1.norm1.bias\n",
      "transformer.layers.1.norm2.weight\n",
      "transformer.layers.1.norm2.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "for name in state_dict:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3449b3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: embedding.weight\n",
      "Shape: [128, 28]\n",
      "Values:\n",
      "tensor([[ 0.0829,  0.0953, -0.0074,  ..., -0.1192, -0.1312,  0.1086],\n",
      "        [ 0.0645,  0.0582, -0.0803,  ..., -0.1500,  0.0287,  0.0968],\n",
      "        [ 0.0590,  0.0028,  0.0488,  ...,  0.0785,  0.0548,  0.1621],\n",
      "        ...,\n",
      "        [ 0.1346,  0.0248, -0.0265,  ..., -0.0672,  0.1621,  0.1520],\n",
      "        [ 0.1193,  0.0423, -0.1733,  ..., -0.0471,  0.1729,  0.0353],\n",
      "        [-0.0630,  0.1092, -0.1341,  ...,  0.1453,  0.0914,  0.1046]])\n",
      "\n",
      "============================================================\n",
      "Layer: embedding.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([-0.0326,  0.1110, -0.0937, -0.0464, -0.0322, -0.1682,  0.1825,  0.0523,\n",
      "        -0.1390,  0.0024, -0.1347,  0.1498, -0.1115, -0.1518,  0.1580, -0.1645,\n",
      "         0.1185, -0.1095, -0.1103,  0.1026,  0.0411, -0.0851,  0.1397, -0.0670,\n",
      "        -0.0605,  0.1278, -0.1706, -0.0491, -0.1318,  0.1542,  0.1217, -0.0843,\n",
      "         0.0858, -0.1206, -0.1659, -0.0159, -0.0402, -0.1030, -0.0226, -0.1466,\n",
      "        -0.1722, -0.0865, -0.1115, -0.1684, -0.0863,  0.1097, -0.0349,  0.0338,\n",
      "         0.0889, -0.1088,  0.0701, -0.0299, -0.0266,  0.0902,  0.1471,  0.0335,\n",
      "         0.0962,  0.0622,  0.0966, -0.0814, -0.1837,  0.1778,  0.1478,  0.0277,\n",
      "         0.1863, -0.0279, -0.1857, -0.1095,  0.0668,  0.1627,  0.1129, -0.1337,\n",
      "         0.1475,  0.0863, -0.1766,  0.1718, -0.1006, -0.0767,  0.0841,  0.1137,\n",
      "         0.1668,  0.0754, -0.0971,  0.1235,  0.1836, -0.0769,  0.0834,  0.1547,\n",
      "         0.0710,  0.0146, -0.1231, -0.0158, -0.1688,  0.1798, -0.0472, -0.0665,\n",
      "        -0.0828,  0.0856, -0.0560,  0.1551, -0.1805, -0.1571,  0.1427,  0.1270,\n",
      "         0.1530, -0.1495,  0.0722,  0.1355, -0.0634, -0.0686,  0.0699,  0.0844,\n",
      "         0.1223,  0.1205,  0.0663,  0.1465,  0.1608, -0.1280, -0.0945,  0.1680,\n",
      "         0.0069,  0.0389,  0.1087, -0.1849, -0.0324,  0.0645, -0.1256,  0.0608])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.self_attn.in_proj_weight\n",
      "Shape: [384, 128]\n",
      "Values:\n",
      "tensor([[ 0.0850,  0.0408,  0.0663,  ..., -0.0799,  0.0942, -0.0431],\n",
      "        [ 0.0030, -0.1077,  0.0532,  ..., -0.0627, -0.0168,  0.0163],\n",
      "        [-0.0239,  0.1053,  0.0121,  ...,  0.0786,  0.0271, -0.0586],\n",
      "        ...,\n",
      "        [-0.0738, -0.0730,  0.1007,  ..., -0.0043, -0.0450, -0.0182],\n",
      "        [-0.0489,  0.0024,  0.0190,  ..., -0.1027,  0.0521, -0.0572],\n",
      "        [-0.0065, -0.0872,  0.0775,  ..., -0.0803,  0.0062,  0.0905]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.self_attn.in_proj_bias\n",
      "Shape: [384]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.self_attn.out_proj.weight\n",
      "Shape: [128, 128]\n",
      "Values:\n",
      "tensor([[-0.0598,  0.0853,  0.0495,  ..., -0.0598, -0.0123, -0.0407],\n",
      "        [-0.0178, -0.0367, -0.0694,  ..., -0.0648, -0.0200, -0.0250],\n",
      "        [-0.0019, -0.0810,  0.0247,  ...,  0.0871, -0.0392,  0.0172],\n",
      "        ...,\n",
      "        [ 0.0258,  0.0827,  0.0433,  ..., -0.0117, -0.0750, -0.0529],\n",
      "        [ 0.0565,  0.0693,  0.0330,  ..., -0.0110,  0.0460,  0.0140],\n",
      "        [ 0.0642, -0.0404,  0.0631,  ..., -0.0191, -0.0346, -0.0079]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.self_attn.out_proj.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.linear1.weight\n",
      "Shape: [256, 128]\n",
      "Values:\n",
      "tensor([[-0.0267,  0.0663, -0.0475,  ..., -0.0049, -0.0750, -0.0771],\n",
      "        [-0.0026, -0.0630,  0.0554,  ..., -0.0637,  0.0336,  0.0001],\n",
      "        [ 0.0794,  0.0158,  0.0523,  ..., -0.0025, -0.0244, -0.0474],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0316, -0.0214,  ...,  0.0353, -0.0129, -0.0282],\n",
      "        [-0.0672, -0.0725, -0.0095,  ...,  0.0243,  0.0585,  0.0676],\n",
      "        [-0.0184,  0.0002, -0.0313,  ..., -0.0805,  0.0762, -0.0382]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.linear1.bias\n",
      "Shape: [256]\n",
      "Values:\n",
      "tensor([-0.0865, -0.0132, -0.0135,  0.0615,  0.0741,  0.0601, -0.0184,  0.0597,\n",
      "        -0.0276, -0.0635, -0.0839,  0.0822,  0.0741, -0.0636, -0.0736,  0.0086,\n",
      "         0.0556,  0.0529, -0.0385, -0.0179,  0.0309,  0.0059,  0.0171, -0.0418,\n",
      "        -0.0620,  0.0814,  0.0206, -0.0686,  0.0707,  0.0633,  0.0859,  0.0612,\n",
      "        -0.0762,  0.0759, -0.0248,  0.0482, -0.0330,  0.0797,  0.0755,  0.0872,\n",
      "         0.0266,  0.0059,  0.0786,  0.0271,  0.0788,  0.0557,  0.0655,  0.0005,\n",
      "        -0.0434,  0.0822,  0.0757,  0.0260, -0.0465,  0.0776, -0.0844, -0.0369,\n",
      "        -0.0363, -0.0418, -0.0021,  0.0836,  0.0749, -0.0124, -0.0837, -0.0666,\n",
      "         0.0677, -0.0800,  0.0275,  0.0119,  0.0142,  0.0175, -0.0020,  0.0196,\n",
      "        -0.0601,  0.0292, -0.0289, -0.0498,  0.0228,  0.0428,  0.0282,  0.0197,\n",
      "         0.0243,  0.0587, -0.0063,  0.0445, -0.0576, -0.0823, -0.0286, -0.0563,\n",
      "         0.0454,  0.0777, -0.0370, -0.0272,  0.0875, -0.0806,  0.0594,  0.0482,\n",
      "        -0.0579,  0.0657,  0.0482, -0.0410, -0.0403, -0.0874,  0.0533, -0.0528,\n",
      "         0.0027,  0.0524, -0.0262,  0.0522, -0.0630,  0.0790,  0.0018,  0.0650,\n",
      "         0.0542, -0.0610, -0.0287, -0.0336,  0.0788,  0.0449, -0.0514, -0.0406,\n",
      "        -0.0493,  0.0715, -0.0717,  0.0126, -0.0380, -0.0584,  0.0783,  0.0769,\n",
      "        -0.0177,  0.0692, -0.0203, -0.0784,  0.0726, -0.0474, -0.0009, -0.0681,\n",
      "        -0.0093, -0.0137, -0.0212,  0.0419,  0.0847,  0.0405, -0.0709,  0.0725,\n",
      "        -0.0198,  0.0031,  0.0402,  0.0171,  0.0348,  0.0443,  0.0405,  0.0635,\n",
      "         0.0362, -0.0821, -0.0015,  0.0684, -0.0326, -0.0728, -0.0574,  0.0214,\n",
      "        -0.0807,  0.0009, -0.0126,  0.0226,  0.0062,  0.0165,  0.0012,  0.0015,\n",
      "        -0.0190,  0.0555,  0.0658,  0.0281, -0.0008,  0.0343, -0.0679, -0.0094,\n",
      "         0.0637, -0.0614, -0.0169, -0.0449,  0.0083, -0.0387, -0.0293, -0.0830,\n",
      "         0.0849,  0.0028,  0.0300, -0.0597,  0.0216, -0.0326,  0.0111,  0.0116,\n",
      "        -0.0031, -0.0572, -0.0810, -0.0031, -0.0749,  0.0812,  0.0619,  0.0633,\n",
      "        -0.0092,  0.0407, -0.0344, -0.0452, -0.0112, -0.0473, -0.0782,  0.0199,\n",
      "        -0.0153,  0.0398,  0.0243, -0.0342,  0.0670,  0.0304,  0.0492,  0.0331,\n",
      "        -0.0875,  0.0611, -0.0793, -0.0357,  0.0215, -0.0055,  0.0643,  0.0825,\n",
      "        -0.0349,  0.0359,  0.0376,  0.0693, -0.0743,  0.0375,  0.0439, -0.0409,\n",
      "        -0.0772,  0.0770,  0.0775,  0.0524, -0.0508,  0.0224, -0.0318,  0.0047,\n",
      "        -0.0780,  0.0499,  0.0115,  0.0870,  0.0073, -0.0131, -0.0444, -0.0101,\n",
      "        -0.0655, -0.0868, -0.0707, -0.0393,  0.0452,  0.0470,  0.0288, -0.0183])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.linear2.weight\n",
      "Shape: [128, 256]\n",
      "Values:\n",
      "tensor([[ 0.0452, -0.0231,  0.0374,  ..., -0.0596,  0.0566,  0.0372],\n",
      "        [-0.0165,  0.0416,  0.0153,  ..., -0.0424, -0.0124, -0.0188],\n",
      "        [-0.0515, -0.0484,  0.0538,  ..., -0.0074, -0.0087,  0.0071],\n",
      "        ...,\n",
      "        [ 0.0384, -0.0226, -0.0183,  ..., -0.0065,  0.0259,  0.0617],\n",
      "        [-0.0006, -0.0110,  0.0395,  ...,  0.0263,  0.0081, -0.0504],\n",
      "        [ 0.0126, -0.0073, -0.0327,  ...,  0.0227, -0.0478,  0.0265]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.linear2.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([-0.0317, -0.0058, -0.0225,  0.0114,  0.0563, -0.0196, -0.0013, -0.0066,\n",
      "         0.0368,  0.0014, -0.0614, -0.0111, -0.0269, -0.0371, -0.0368,  0.0087,\n",
      "         0.0161, -0.0361, -0.0319,  0.0045,  0.0477, -0.0133, -0.0203,  0.0485,\n",
      "        -0.0254,  0.0115, -0.0133, -0.0086,  0.0273,  0.0316, -0.0302,  0.0601,\n",
      "         0.0597, -0.0099,  0.0198,  0.0401,  0.0146,  0.0274, -0.0392, -0.0612,\n",
      "         0.0608,  0.0075,  0.0286, -0.0574,  0.0419,  0.0492,  0.0380, -0.0264,\n",
      "        -0.0089, -0.0331,  0.0502, -0.0238, -0.0293, -0.0175,  0.0323,  0.0322,\n",
      "        -0.0377, -0.0346,  0.0491,  0.0244, -0.0080,  0.0106,  0.0392,  0.0473,\n",
      "        -0.0611, -0.0355, -0.0562,  0.0215,  0.0466,  0.0614, -0.0356, -0.0306,\n",
      "         0.0170, -0.0080, -0.0008, -0.0471, -0.0498,  0.0006,  0.0027, -0.0523,\n",
      "         0.0481,  0.0367, -0.0556, -0.0401,  0.0206,  0.0091, -0.0463, -0.0344,\n",
      "         0.0120,  0.0279, -0.0278,  0.0344, -0.0563, -0.0039,  0.0027, -0.0424,\n",
      "         0.0019, -0.0244, -0.0156,  0.0038,  0.0033,  0.0422,  0.0496, -0.0466,\n",
      "        -0.0212, -0.0242,  0.0617, -0.0019, -0.0178,  0.0034, -0.0424, -0.0300,\n",
      "         0.0156,  0.0365, -0.0277,  0.0280, -0.0303, -0.0216, -0.0368, -0.0033,\n",
      "         0.0050,  0.0454, -0.0308,  0.0290, -0.0091,  0.0505, -0.0171,  0.0350])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.norm1.weight\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.norm1.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.norm2.weight\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.0.norm2.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.self_attn.in_proj_weight\n",
      "Shape: [384, 128]\n",
      "Values:\n",
      "tensor([[ 0.0850,  0.0408,  0.0663,  ..., -0.0799,  0.0942, -0.0431],\n",
      "        [ 0.0030, -0.1077,  0.0532,  ..., -0.0627, -0.0168,  0.0163],\n",
      "        [-0.0239,  0.1053,  0.0121,  ...,  0.0786,  0.0271, -0.0586],\n",
      "        ...,\n",
      "        [-0.0738, -0.0730,  0.1007,  ..., -0.0043, -0.0450, -0.0182],\n",
      "        [-0.0489,  0.0024,  0.0190,  ..., -0.1027,  0.0521, -0.0572],\n",
      "        [-0.0065, -0.0872,  0.0775,  ..., -0.0803,  0.0062,  0.0905]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.self_attn.in_proj_bias\n",
      "Shape: [384]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.self_attn.out_proj.weight\n",
      "Shape: [128, 128]\n",
      "Values:\n",
      "tensor([[-0.0598,  0.0853,  0.0495,  ..., -0.0598, -0.0123, -0.0407],\n",
      "        [-0.0178, -0.0367, -0.0694,  ..., -0.0648, -0.0200, -0.0250],\n",
      "        [-0.0019, -0.0810,  0.0247,  ...,  0.0871, -0.0392,  0.0172],\n",
      "        ...,\n",
      "        [ 0.0258,  0.0827,  0.0433,  ..., -0.0117, -0.0750, -0.0529],\n",
      "        [ 0.0565,  0.0693,  0.0330,  ..., -0.0110,  0.0460,  0.0140],\n",
      "        [ 0.0642, -0.0404,  0.0631,  ..., -0.0191, -0.0346, -0.0079]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.self_attn.out_proj.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.linear1.weight\n",
      "Shape: [256, 128]\n",
      "Values:\n",
      "tensor([[-0.0267,  0.0663, -0.0475,  ..., -0.0049, -0.0750, -0.0771],\n",
      "        [-0.0026, -0.0630,  0.0554,  ..., -0.0637,  0.0336,  0.0001],\n",
      "        [ 0.0794,  0.0158,  0.0523,  ..., -0.0025, -0.0244, -0.0474],\n",
      "        ...,\n",
      "        [ 0.0062,  0.0316, -0.0214,  ...,  0.0353, -0.0129, -0.0282],\n",
      "        [-0.0672, -0.0725, -0.0095,  ...,  0.0243,  0.0585,  0.0676],\n",
      "        [-0.0184,  0.0002, -0.0313,  ..., -0.0805,  0.0762, -0.0382]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.linear1.bias\n",
      "Shape: [256]\n",
      "Values:\n",
      "tensor([-0.0865, -0.0132, -0.0135,  0.0615,  0.0741,  0.0601, -0.0184,  0.0597,\n",
      "        -0.0276, -0.0635, -0.0839,  0.0822,  0.0741, -0.0636, -0.0736,  0.0086,\n",
      "         0.0556,  0.0529, -0.0385, -0.0179,  0.0309,  0.0059,  0.0171, -0.0418,\n",
      "        -0.0620,  0.0814,  0.0206, -0.0686,  0.0707,  0.0633,  0.0859,  0.0612,\n",
      "        -0.0762,  0.0759, -0.0248,  0.0482, -0.0330,  0.0797,  0.0755,  0.0872,\n",
      "         0.0266,  0.0059,  0.0786,  0.0271,  0.0788,  0.0557,  0.0655,  0.0005,\n",
      "        -0.0434,  0.0822,  0.0757,  0.0260, -0.0465,  0.0776, -0.0844, -0.0369,\n",
      "        -0.0363, -0.0418, -0.0021,  0.0836,  0.0749, -0.0124, -0.0837, -0.0666,\n",
      "         0.0677, -0.0800,  0.0275,  0.0119,  0.0142,  0.0175, -0.0020,  0.0196,\n",
      "        -0.0601,  0.0292, -0.0289, -0.0498,  0.0228,  0.0428,  0.0282,  0.0197,\n",
      "         0.0243,  0.0587, -0.0063,  0.0445, -0.0576, -0.0823, -0.0286, -0.0563,\n",
      "         0.0454,  0.0777, -0.0370, -0.0272,  0.0875, -0.0806,  0.0594,  0.0482,\n",
      "        -0.0579,  0.0657,  0.0482, -0.0410, -0.0403, -0.0874,  0.0533, -0.0528,\n",
      "         0.0027,  0.0524, -0.0262,  0.0522, -0.0630,  0.0790,  0.0018,  0.0650,\n",
      "         0.0542, -0.0610, -0.0287, -0.0336,  0.0788,  0.0449, -0.0514, -0.0406,\n",
      "        -0.0493,  0.0715, -0.0717,  0.0126, -0.0380, -0.0584,  0.0783,  0.0769,\n",
      "        -0.0177,  0.0692, -0.0203, -0.0784,  0.0726, -0.0474, -0.0009, -0.0681,\n",
      "        -0.0093, -0.0137, -0.0212,  0.0419,  0.0847,  0.0405, -0.0709,  0.0725,\n",
      "        -0.0198,  0.0031,  0.0402,  0.0171,  0.0348,  0.0443,  0.0405,  0.0635,\n",
      "         0.0362, -0.0821, -0.0015,  0.0684, -0.0326, -0.0728, -0.0574,  0.0214,\n",
      "        -0.0807,  0.0009, -0.0126,  0.0226,  0.0062,  0.0165,  0.0012,  0.0015,\n",
      "        -0.0190,  0.0555,  0.0658,  0.0281, -0.0008,  0.0343, -0.0679, -0.0094,\n",
      "         0.0637, -0.0614, -0.0169, -0.0449,  0.0083, -0.0387, -0.0293, -0.0830,\n",
      "         0.0849,  0.0028,  0.0300, -0.0597,  0.0216, -0.0326,  0.0111,  0.0116,\n",
      "        -0.0031, -0.0572, -0.0810, -0.0031, -0.0749,  0.0812,  0.0619,  0.0633,\n",
      "        -0.0092,  0.0407, -0.0344, -0.0452, -0.0112, -0.0473, -0.0782,  0.0199,\n",
      "        -0.0153,  0.0398,  0.0243, -0.0342,  0.0670,  0.0304,  0.0492,  0.0331,\n",
      "        -0.0875,  0.0611, -0.0793, -0.0357,  0.0215, -0.0055,  0.0643,  0.0825,\n",
      "        -0.0349,  0.0359,  0.0376,  0.0693, -0.0743,  0.0375,  0.0439, -0.0409,\n",
      "        -0.0772,  0.0770,  0.0775,  0.0524, -0.0508,  0.0224, -0.0318,  0.0047,\n",
      "        -0.0780,  0.0499,  0.0115,  0.0870,  0.0073, -0.0131, -0.0444, -0.0101,\n",
      "        -0.0655, -0.0868, -0.0707, -0.0393,  0.0452,  0.0470,  0.0288, -0.0183])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.linear2.weight\n",
      "Shape: [128, 256]\n",
      "Values:\n",
      "tensor([[ 0.0452, -0.0231,  0.0374,  ..., -0.0596,  0.0566,  0.0372],\n",
      "        [-0.0165,  0.0416,  0.0153,  ..., -0.0424, -0.0124, -0.0188],\n",
      "        [-0.0515, -0.0484,  0.0538,  ..., -0.0074, -0.0087,  0.0071],\n",
      "        ...,\n",
      "        [ 0.0384, -0.0226, -0.0183,  ..., -0.0065,  0.0259,  0.0617],\n",
      "        [-0.0006, -0.0110,  0.0395,  ...,  0.0263,  0.0081, -0.0504],\n",
      "        [ 0.0126, -0.0073, -0.0327,  ...,  0.0227, -0.0478,  0.0265]])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.linear2.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([-0.0317, -0.0058, -0.0225,  0.0114,  0.0563, -0.0196, -0.0013, -0.0066,\n",
      "         0.0368,  0.0014, -0.0614, -0.0111, -0.0269, -0.0371, -0.0368,  0.0087,\n",
      "         0.0161, -0.0361, -0.0319,  0.0045,  0.0477, -0.0133, -0.0203,  0.0485,\n",
      "        -0.0254,  0.0115, -0.0133, -0.0086,  0.0273,  0.0316, -0.0302,  0.0601,\n",
      "         0.0597, -0.0099,  0.0198,  0.0401,  0.0146,  0.0274, -0.0392, -0.0612,\n",
      "         0.0608,  0.0075,  0.0286, -0.0574,  0.0419,  0.0492,  0.0380, -0.0264,\n",
      "        -0.0089, -0.0331,  0.0502, -0.0238, -0.0293, -0.0175,  0.0323,  0.0322,\n",
      "        -0.0377, -0.0346,  0.0491,  0.0244, -0.0080,  0.0106,  0.0392,  0.0473,\n",
      "        -0.0611, -0.0355, -0.0562,  0.0215,  0.0466,  0.0614, -0.0356, -0.0306,\n",
      "         0.0170, -0.0080, -0.0008, -0.0471, -0.0498,  0.0006,  0.0027, -0.0523,\n",
      "         0.0481,  0.0367, -0.0556, -0.0401,  0.0206,  0.0091, -0.0463, -0.0344,\n",
      "         0.0120,  0.0279, -0.0278,  0.0344, -0.0563, -0.0039,  0.0027, -0.0424,\n",
      "         0.0019, -0.0244, -0.0156,  0.0038,  0.0033,  0.0422,  0.0496, -0.0466,\n",
      "        -0.0212, -0.0242,  0.0617, -0.0019, -0.0178,  0.0034, -0.0424, -0.0300,\n",
      "         0.0156,  0.0365, -0.0277,  0.0280, -0.0303, -0.0216, -0.0368, -0.0033,\n",
      "         0.0050,  0.0454, -0.0308,  0.0290, -0.0091,  0.0505, -0.0171,  0.0350])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.norm1.weight\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.norm1.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.norm2.weight\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1.])\n",
      "\n",
      "============================================================\n",
      "Layer: transformer.layers.1.norm2.bias\n",
      "Shape: [128]\n",
      "Values:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "============================================================\n",
      "Layer: classifier.weight\n",
      "Shape: [10, 128]\n",
      "Values:\n",
      "tensor([[-0.0202, -0.0495,  0.0176,  ..., -0.0110, -0.0218, -0.0125],\n",
      "        [-0.0132,  0.0510, -0.0271,  ..., -0.0187, -0.0124,  0.0230],\n",
      "        [-0.0303, -0.0005, -0.0625,  ..., -0.0646, -0.0411,  0.0543],\n",
      "        ...,\n",
      "        [ 0.0532, -0.0339,  0.0383,  ..., -0.0049, -0.0356, -0.0729],\n",
      "        [ 0.0585,  0.0050, -0.0584,  ...,  0.0803, -0.0146, -0.0497],\n",
      "        [-0.0682,  0.0581, -0.0775,  ...,  0.0809,  0.0486, -0.0618]])\n",
      "\n",
      "============================================================\n",
      "Layer: classifier.bias\n",
      "Shape: [10]\n",
      "Values:\n",
      "tensor([-0.0710,  0.0869, -0.0378,  0.0810,  0.0552,  0.0440,  0.0188, -0.0674,\n",
      "         0.0146,  0.0628])\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "for name, param in state_dict.items():\n",
    "    print(f\"Layer: {name}\")\n",
    "    print(f\"Shape: {list(param.shape)}\")\n",
    "    print(f\"Values:\\n{param}\\n\")\n",
    "    print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cd38d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"full_precision_parameters.txt\", \"w\") as f:\n",
    "    for name, param in state_dict.items():\n",
    "        f.write(f\"Layer: {name}\\n\")\n",
    "        f.write(f\"Shape: {list(param.shape)}\\n\")\n",
    "        f.write(f\"Values:\\n{param.cpu().numpy()}\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "036e5990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNISTTransformer(\n",
      "  (embedding): Linear(in_features=28, out_features=128, bias=True)\n",
      "  (pos_enc): PositionalEncoding()\n",
      "  (transformer): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "28930f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.weight                         | dtype: torch.float32\n",
      "embedding.bias                           | dtype: torch.float32\n",
      "transformer.layers.0.self_attn.in_proj_weight | dtype: torch.float32\n",
      "transformer.layers.0.self_attn.in_proj_bias | dtype: torch.float32\n",
      "transformer.layers.0.self_attn.out_proj.weight | dtype: torch.float32\n",
      "transformer.layers.0.self_attn.out_proj.bias | dtype: torch.float32\n",
      "transformer.layers.0.linear1.weight      | dtype: torch.float32\n",
      "transformer.layers.0.linear1.bias        | dtype: torch.float32\n",
      "transformer.layers.0.linear2.weight      | dtype: torch.float32\n",
      "transformer.layers.0.linear2.bias        | dtype: torch.float32\n",
      "transformer.layers.0.norm1.weight        | dtype: torch.float32\n",
      "transformer.layers.0.norm1.bias          | dtype: torch.float32\n",
      "transformer.layers.0.norm2.weight        | dtype: torch.float32\n",
      "transformer.layers.0.norm2.bias          | dtype: torch.float32\n",
      "transformer.layers.1.self_attn.in_proj_weight | dtype: torch.float32\n",
      "transformer.layers.1.self_attn.in_proj_bias | dtype: torch.float32\n",
      "transformer.layers.1.self_attn.out_proj.weight | dtype: torch.float32\n",
      "transformer.layers.1.self_attn.out_proj.bias | dtype: torch.float32\n",
      "transformer.layers.1.linear1.weight      | dtype: torch.float32\n",
      "transformer.layers.1.linear1.bias        | dtype: torch.float32\n",
      "transformer.layers.1.linear2.weight      | dtype: torch.float32\n",
      "transformer.layers.1.linear2.bias        | dtype: torch.float32\n",
      "transformer.layers.1.norm1.weight        | dtype: torch.float32\n",
      "transformer.layers.1.norm1.bias          | dtype: torch.float32\n",
      "transformer.layers.1.norm2.weight        | dtype: torch.float32\n",
      "transformer.layers.1.norm2.bias          | dtype: torch.float32\n",
      "classifier.weight                        | dtype: torch.float32\n",
      "classifier.bias                          | dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name:40s} | dtype: {param.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "hi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
